{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM-SR\n",
    "\n",
    "*Sign recognition demo using the BrainForge library*\n",
    "\n",
    "**Author**: Csaba GÃ³r\n",
    "\n",
    "This notebook illustrates the usage of the *BrainForge* Neural Network library. The library can be obtained by issuing the following *pip* command:\n",
    "\n",
    "```pip install git+https://github.com/csxeba/brainforge.git```\n",
    "\n",
    "Since this demonstration also depends on other packages, a *conda* environment descriptor *YaML* file is supplied (*env.yml*). This environment can be set up by issuing the following *conda* command:\n",
    "\n",
    "```conda env create -f env.yml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from brainforge import Backpropagation, LayerStack\n",
    "from brainforge import layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset which is going to be fit is a **road sign recognition** dataset, which can either be downloaded and extracted to this project's *data/* folder using the *data/get.sh* script or the dataset root can also be specified below if the dataset is already available.\n",
    "\n",
    "The model performance will be monitored on a validation subset, which is a 20% split from the training set. The validation set is always the last 20% of the images of every class, determined by the increasing sorting order of their file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"data/train-52x52\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Streamer] - Num train samples: 48000\n",
      " [Streamer] - Num val samples: 12000\n"
     ]
    }
   ],
   "source": [
    "import streamer\n",
    "\n",
    "stream = streamer.Stream(root=DATASET_ROOT, split_validation=0.2, image_format=\"channels_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The data is fit by an *Artificial Neural Network*, more specifically a *Fully Convolutional Neural Network*, which has a relatively low number of parameters and thus (hopefully) generalizes better than a classic CNN with a Dense head.\n",
    "\n",
    "The network weights are optimized by *Stochastic Gradient Descent* on the gradients determined by *Backpropagation*. The model output activation and loss functions are chosen so that they reflect the *multiclass classification* nature of the problem. The optimizer is chosen to be the *Adam* optimizer [Kigma & Ba, 2015](https://arxiv.org/abs/1412.6980), which is more-or-less a default choice for the optimizer and tends to perform adequatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stack = LayerStack(stream.input_shape, layers=[\n",
    "    \n",
    "    layers.ConvLayer(nfilters=16, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=stream.NUM_CLASSES, filterx=5, filtery=5, compiled=True),\n",
    "\n",
    "    layers.GlobalAveragePooling(),\n",
    "    layers.Activation(\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training takes about 1.5 hours and the network reaches over 99.9% accuracy on the validation set, which is unnaturally high and is caused probably by the fact that the validation set is highly similar to the training set.\n",
    "\n",
    "Below are the parameters for the training. Previous experiments showed that 6 epochs are sufficient to reach convergence on this dataset. The relatively low batch size and high learning rate has ensures the network jumps out of smaller local minima and finds a good optimum with good generalization. Together with the fully convolutional nature of the architecture, this produces sufficient regularization, so no additional regularization was required.\n",
    "\n",
    "A validation increase factor is applied to better monitor the development of the target KPI, which is the classification accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "VALIDATION_INCREASE_FACTOR = 4  # divides steps per epoch and multiplies epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/24\n",
      "Training Progress: 100.0%  cost: 0.9391 accuracy: 0.6517 Validation cost: 0.5285 accuracy: 0.8100\n",
      " took 3.84 minutes\n",
      "Epoch  2/24\n",
      "Training Progress: 100.0%  cost: 0.4122 accuracy: 0.8565 Validation cost: 0.3029 accuracy: 0.9100\n",
      " took 3.83 minutes\n",
      "Epoch  3/24\n",
      "Training Progress: 100.0%  cost: 0.2511 accuracy: 0.9214 Validation cost: 0.2248 accuracy: 0.9324\n",
      " took 3.82 minutes\n",
      "Epoch  4/24\n",
      "Training Progress: 100.0%  cost: 0.1684 accuracy: 0.9523 Validation cost: 0.1382 accuracy: 0.9613\n",
      " took 3.82 minutes\n",
      "Epoch  5/24\n",
      "Training Progress: 100.0%  cost: 0.1240 accuracy: 0.9650 Validation cost: 0.1063 accuracy: 0.9733\n",
      " took 3.82 minutes\n",
      "Epoch  6/24\n",
      "Training Progress: 100.0%  cost: 0.1032 accuracy: 0.9722 Validation cost: 0.0919 accuracy: 0.9764\n",
      " took 3.82 minutes\n",
      "Epoch  7/24\n",
      "Training Progress: 100.0%  cost: 0.0794 accuracy: 0.9784 Validation cost: 0.0822 accuracy: 0.9814\n",
      " took 3.82 minutes\n",
      "Epoch  8/24\n",
      "Training Progress: 100.0%  cost: 0.0599 accuracy: 0.9849 Validation cost: 0.0564 accuracy: 0.9826\n",
      " took 3.82 minutes\n",
      "Epoch  9/24\n",
      "Training Progress: 100.0%  cost: 0.0504 accuracy: 0.9888 Validation cost: 0.0431 accuracy: 0.9911\n",
      " took 3.85 minutes\n",
      "Epoch 10/24\n",
      "Training Progress: 100.0%  cost: 0.0434 accuracy: 0.9907 Validation cost: 0.0323 accuracy: 0.9932\n",
      " took 3.83 minutes\n",
      "Epoch 11/24\n",
      "Training Progress: 100.0%  cost: 0.0319 accuracy: 0.9921 Validation cost: 0.0319 accuracy: 0.9930\n",
      " took 3.84 minutes\n",
      "Epoch 12/24\n",
      "Training Progress: 100.0%  cost: 0.0292 accuracy: 0.9932 Validation cost: 0.0280 accuracy: 0.9952\n",
      " took 3.84 minutes\n",
      "Epoch 13/24\n",
      "Training Progress: 100.0%  cost: 0.0225 accuracy: 0.9948 Validation cost: 0.0273 accuracy: 0.9948\n",
      " took 3.84 minutes\n",
      "Epoch 14/24\n",
      "Training Progress: 100.0%  cost: 0.0252 accuracy: 0.9949 Validation cost: 0.0421 accuracy: 0.9878\n",
      " took 3.84 minutes\n",
      "Epoch 15/24\n",
      "Training Progress: 100.0%  cost: 0.0187 accuracy: 0.9952 Validation cost: 0.0309 accuracy: 0.9919\n",
      " took 3.87 minutes\n",
      "Epoch 16/24\n",
      "Training Progress: 100.0%  cost: 0.0202 accuracy: 0.9959 Validation cost: 0.0220 accuracy: 0.9968\n",
      " took 3.82 minutes\n",
      "Epoch 17/24\n",
      "Training Progress: 100.0%  cost: 0.0147 accuracy: 0.9968 Validation cost: 0.0193 accuracy: 0.9959\n",
      " took 3.82 minutes\n",
      "Epoch 18/24\n",
      "Training Progress: 100.0%  cost: 0.0159 accuracy: 0.9966 Validation cost: 0.0160 accuracy: 0.9971\n",
      " took 3.83 minutes\n",
      "Epoch 19/24\n",
      "Training Progress: 100.0%  cost: 0.0168 accuracy: 0.9962 Validation cost: 0.0182 accuracy: 0.9962\n",
      " took 3.81 minutes\n",
      "Epoch 20/24\n",
      "Training Progress: 100.0%  cost: 0.0116 accuracy: 0.9973 Validation cost: 0.0195 accuracy: 0.9960\n",
      " took 3.83 minutes\n",
      "Epoch 21/24\n",
      "Training Progress: 100.0%  cost: 0.0117 accuracy: 0.9976 Validation cost: 0.0331 accuracy: 0.9900\n",
      " took 3.83 minutes\n",
      "Epoch 22/24\n",
      "Training Progress: 100.0%  cost: 0.0103 accuracy: 0.9978 Validation cost: 0.0134 accuracy: 0.9971\n",
      " took 3.82 minutes\n",
      "Epoch 23/24\n",
      "Training Progress: 100.0%  cost: 0.0092 accuracy: 0.9974 Validation cost: 0.0111 accuracy: 0.9982\n",
      " took 3.82 minutes\n",
      "Epoch 24/24\n",
      "Training Progress: 100.0%  cost: 0.0112 accuracy: 0.9970 Validation cost: 0.0157 accuracy: 0.9968\n",
      " took 3.83 minutes\n"
     ]
    }
   ],
   "source": [
    "trainer = Backpropagation(layerstack=stack, cost=\"cxent\", optimizer=optimizers.Adam(LEARNING_RATE))\n",
    "\n",
    "trainer.fit_generator(stream.iter_subset(\"train\", BATCH_SIZE),\n",
    "                      lessons_per_epoch=stream.steps_per_epoch(\"train\", BATCH_SIZE) // VALIDATION_INCREASE_FACTOR,\n",
    "                      epochs=EPOCHS * VALIDATION_INCREASE_FACTOR,\n",
    "                      metrics=[\"acc\"],\n",
    "                      validation=stream.iter_subset(\"val\", BATCH_SIZE),\n",
    "                      validation_steps=stream.steps_per_epoch(\"val\", BATCH_SIZE))\n",
    "\n",
    "# Save the weights as NumPy vector.\n",
    "weights = stack.get_weights(unfold=True)\n",
    "np.save(\"AIM-SR-weights.npy\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying early stopping, regularization, augmentation, batch normalization and bigger architectures was considered, but rejected, because the score is already high enough, so no additional complexity was required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Below we set up some functions to aid testing the network on arbitrary input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_WEIGHTS = \"AIM-SR-weights.npy\"\n",
    "\n",
    "stack.set_weights(np.load(NETWORK_WEIGHTS), fold=True)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    x = image / 255.  # Downscale to range 0. - 1.\n",
    "    x = x.transpose((2, 0, 1))  # Convert to channels first\n",
    "    return x[None, ...]  # Add a batch dimension\n",
    "\n",
    "def execute_detection(image: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Runs preprocessing, executes the network and returns an integer label.\n",
    "    Returned labels are indexed from 1, just like in the dataset.\n",
    "    \n",
    "    image: np.ndarray\n",
    "        Single BGR image as a 3D numpy array in channels last format.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = preprocess_image(image)\n",
    "    output = stack.feedforward(x)[0]  # eliminate batch dim\n",
    "    prediction = np.argmax(output) + 1\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = str(stream.paths[-1])\n",
    "image = cv2.imread(str(stream.paths[-1]))\n",
    "label = stream.labels[-1] + 1\n",
    "\n",
    "prediction = execute_detection(image)\n",
    "\n",
    "print(f\"Label: {label} Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
