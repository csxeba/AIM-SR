{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM-SR\n",
    "\n",
    "*Sign recognition demo using the BrainForge library*\n",
    "\n",
    "**Author**: Csaba GÃ³r\n",
    "\n",
    "This notebook illustrates the usage of the *BrainForge* Neural Network library. The library can be obtained by issuing the following *pip* command:\n",
    "\n",
    "```pip install git+https://github.com/csxeba/brainforge.git```\n",
    "\n",
    "Since this demonstration also depends on other packages, a *conda* environment descriptor *YaML* file is supplied (*env.yml*). This environment can be set up by issuing the following *conda* command:\n",
    "\n",
    "```conda env create -f env.yml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from brainforge import Backpropagation, LayerStack\n",
    "from brainforge import layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset which is going to be fit is a **road sign recognition** dataset, which can either be downloaded and extracted to this project's *data/* folder using the *data/get.sh* script or the dataset root can also be specified below if the dataset is already available.\n",
    "\n",
    "The model performance will be monitored on a validation subset, which is a 20% split from the training set. The validation set is always the last 20% of the images of every class, determined by the increasing sorting order of their file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"data/train-52x52\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import streamer\n",
    "\n",
    "stream = streamer.Stream(root=DATASET_ROOT, split_validation=0.2, image_format=\"channels_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The data is fit by an *Artificial Neural Network*, more specifically a *Fully Convolutional Neural Network*, which has a relatively low number of parameters and thus (hopefully) generalizes better than a classic CNN with a Dense head.\n",
    "\n",
    "The network weights are optimized by *Stochastic Gradient Descent* on the gradients determined by *Backpropagation*. The model output activation and loss functions are chosen so that they reflect the *multiclass classification* nature of the problem. The optimizer is chosen to be the *Adam* optimizer [Kigma & Ba, 2015](https://arxiv.org/abs/1412.6980), which is more-or-less a default choice for the optimizer and tends to perform adequatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stack = LayerStack(stream.input_shape, layers=[\n",
    "    \n",
    "    layers.ConvLayer(nfilters=16, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=stream.NUM_CLASSES, filterx=5, filtery=5, compiled=True),\n",
    "\n",
    "    layers.GlobalAveragePooling(),\n",
    "    layers.Activation(\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training takes about 1.5 hours and the network reaches over 99.9% accuracy on the validation set, which is unnaturally high and is caused probably by the fact that the validation set is highly similar to the training set.\n",
    "\n",
    "Below are the parameters for the training. Previous experiments showed that 6 epochs are sufficient to reach convergence on this dataset. The relatively low batch size and high learning rate has ensures the network jumps out of smaller local minima and finds a good optimum with good generalization. Together with the fully convolutional nature of the architecture, this produces sufficient regularization, so no additional regularization was required.\n",
    "\n",
    "A validation increase factor is applied to better monitor the development of the target KPI, which is the classification accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "VALIDATION_INCREASE_FACTOR = 4  # divides steps per epoch and multiplies epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/24\n",
      "Training Progress: 100.0%  cost: 0.8197 accuracy: 0.6938 Validation cost: 0.4722 accuracy: 0.8012\n",
      " took 3.0 minutes\n",
      "Epoch  2/24\n",
      "Training Progress: 100.0%  cost: 0.3428 accuracy: 0.8802 Validation cost: 0.2664 accuracy: 0.9108\n",
      " took 3.0 minutes\n",
      "Epoch  3/24\n",
      "Training Progress: 100.0%  cost: 0.2280 accuracy: 0.9245 Validation cost: 0.2057 accuracy: 0.9420\n",
      " took 3.0 minutes\n",
      "Epoch  4/24\n",
      "Training Progress: 100.0%  cost: 0.1499 accuracy: 0.9583 Validation cost: 0.1402 accuracy: 0.9607\n",
      " took 3.0 minutes\n",
      "Epoch  5/24\n",
      "Training Progress: 100.0%  cost: 0.1073 accuracy: 0.9710 Validation cost: 0.0856 accuracy: 0.9761\n",
      " took 3.0 minutes\n",
      "Epoch  6/24\n",
      "Training Progress: 100.0%  cost: 0.0777 accuracy: 0.9828 Validation cost: 0.0753 accuracy: 0.9806\n",
      " took 3.0 minutes\n",
      "Epoch  7/24\n",
      "Training Progress: 100.0%  cost: 0.0619 accuracy: 0.9844 Validation cost: 0.0518 accuracy: 0.9879\n",
      " took 3.0 minutes\n",
      "Epoch  8/24\n",
      "Training Progress: 100.0%  cost: 0.0475 accuracy: 0.9898 Validation cost: 0.0366 accuracy: 0.9928\n",
      " took 3.0 minutes\n",
      "Epoch  9/24\n",
      "Training Progress: 100.0%  cost: 0.0431 accuracy: 0.9918 Validation cost: 0.1685 accuracy: 0.9385\n",
      " took 3.0 minutes\n",
      "Epoch 10/24\n",
      "Training Progress: 100.0%  cost: 0.0398 accuracy: 0.9916 Validation cost: 0.0311 accuracy: 0.9918\n",
      " took 3.0 minutes\n",
      "Epoch 11/24\n",
      "Training Progress: 100.0%  cost: 0.0305 accuracy: 0.9933 Validation cost: 0.0249 accuracy: 0.9958\n",
      " took 3.0 minutes\n",
      "Epoch 12/24\n",
      "Training Progress: 100.0%  cost: 0.0257 accuracy: 0.9942 Validation cost: 0.0264 accuracy: 0.9941\n",
      " took 3.0 minutes\n",
      "Epoch 13/24\n",
      "Training Progress: 100.0%  cost: 0.0183 accuracy: 0.9965 Validation cost: 0.0146 accuracy: 0.9962\n",
      " took 3.0 minutes\n",
      "Epoch 14/24\n",
      "Training Progress: 100.0%  cost: 0.0203 accuracy: 0.9958 Validation cost: 0.0145 accuracy: 0.9973\n",
      " took 3.0 minutes\n",
      "Epoch 15/24\n",
      "Training Progress: 100.0%  cost: 0.0190 accuracy: 0.9962 Validation cost: 0.0147 accuracy: 0.9975\n",
      " took 3.0 minutes\n",
      "Epoch 16/24\n",
      "Training Progress: 100.0%  cost: 0.0130 accuracy: 0.9972 Validation cost: 0.0127 accuracy: 0.9969\n",
      " took 3.0 minutes\n",
      "Epoch 17/24\n",
      "Training Progress: 100.0%  cost: 0.0131 accuracy: 0.9969 Validation cost: 0.0107 accuracy: 0.9980\n",
      " took 3.0 minutes\n",
      "Epoch 18/24\n",
      "Training Progress: 100.0%  cost: 0.0098 accuracy: 0.9971 Validation cost: 0.0072 accuracy: 0.9987\n",
      " took 3.0 minutes\n",
      "Epoch 19/24\n",
      "Training Progress: 100.0%  cost: 0.0118 accuracy: 0.9973 Validation cost: 0.0089 accuracy: 0.9978\n",
      " took 3.0 minutes\n",
      "Epoch 20/24\n",
      "Training Progress:  23.8%  cost: 0.0040 accuracy: 0.9997"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ef1f5a258d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                   validation_steps=stream.steps_per_epoch(\"val\", BATCH_SIZE))\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Save the weights as NumPy vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aim/lib/python3.7/site-packages/brainforge/learner/abstract_learner.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, lessons_per_epoch, epochs, metrics, validation, validation_steps, verbose, **kw)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {:>{w}}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             epoch_history = self.epoch(generator, updates_per_epoch=lessons_per_epoch, metrics=metrics,\n\u001b[0;32m---> 36\u001b[0;31m                                        validation=validation, validation_steps=validation_steps, verbose=verbose, **kw)\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aim/lib/python3.7/site-packages/brainforge/learner/abstract_learner.py\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(self, generator, updates_per_epoch, metrics, validation, validation_steps, verbose, **kw)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mepoch_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aim/lib/python3.7/site-packages/brainforge/learner/backpropagation.py\u001b[0m in \u001b[0;36mlearn_batch\u001b[0;34m(self, X, Y, w, metrics, update)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aim/lib/python3.7/site-packages/brainforge/learner/backpropagation.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aim/lib/python3.7/site-packages/brainforge/layers/tensor.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, delta)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Backpropagation(layerstack=stack, cost=\"cxent\", optimizer=optimizers.Adam(LEARNING_RATE))\n",
    "\n",
    "net.fit_generator(stream.iter_subset(\"train\", BATCH_SIZE),\n",
    "                  lessons_per_epoch=stream.steps_per_epoch(\"train\", BATCH_SIZE) // VALIDATION_INCREASE_FACTOR,\n",
    "                  epochs=EPOCHS * VALIDATION_INCREASE_FACTOR,\n",
    "                  metrics=[\"acc\"],\n",
    "                  validation=stream.iter_subset(\"val\", BATCH_SIZE),\n",
    "                  validation_steps=stream.steps_per_epoch(\"val\", BATCH_SIZE))\n",
    "\n",
    "# Save the weights as NumPy vector.\n",
    "weights = net.get_weights(unfold=True)\n",
    "np.save(\"AIM-SR-weights.npy\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Below we set up some functions to aid testing the network on arbitrary input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_WEIGHTS = \"AIM-SR-weights.npy\"\n",
    "\n",
    "stack.set_weights(np.load(NETWORK_WEIGHTS), fold=True)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    x = image / 255.  # Downscale to range 0. - 1.\n",
    "    x = x.transpose((2, 0, 1))  # Convert to channels first\n",
    "    return x[None, ...]  # Add a batch dimension\n",
    "\n",
    "def execute_detection(image: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Runs preprocessing, executes the network and returns an integer label.\n",
    "    Returned labels are indexed from 1, just like in the dataset.\n",
    "    \n",
    "    image: np.ndarray\n",
    "        Single BGR image as a 3D numpy array in channels last format.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = preprocess_image(image)\n",
    "    output = stack.feedforward(x)[0]  # eliminate batch dim\n",
    "    prediction = np.argmax(output, axis=1) + 1\n",
    "    return prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
