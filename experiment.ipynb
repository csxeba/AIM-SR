{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM-SR\n",
    "\n",
    "Sign recognition demo using the BrainForge library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'brainforge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6213b044bf4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbrainforge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackpropagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerStack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbrainforge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'brainforge'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from brainforge import Backpropagation, LayerStack\n",
    "from brainforge import layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset can either be downloaded and extracted to this project's data/ folder using the data/get.sh script or you can specify the dataset root below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"data/train-52x52\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stream = streamer.Stream(root=DATASET_ROOT, split_validation=0.2, image_format=\"channels_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The data will be fit by an Artificial Neural Network, more specifically a Fully Convolutional Neural Network, which has a relatively low number of parameters and thus (hopefully) generalizes better than a vanilla CNN with a Dense head.\n",
    "\n",
    "The network weights will be optimized by Gradient Descent on the gradients determined by Backpropagation.\n",
    "\n",
    "The task is a 12-clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stack = LayerStack(stream.input_shape, layers=[\n",
    "    \n",
    "    layers.ConvLayer(nfilters=16, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=stream.NUM_CLASSES, filterx=5, filtery=5, compiled=True),\n",
    "\n",
    "    layers.GlobalAveragePooling(),\n",
    "    layers.Activation(\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"data/train-52x52\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains the implementation for the data reader. Do check it out!\n",
    "import streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = streamer.Stream(root=DATASET_ROOT, split_validation=0.2, image_format=\"channels_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The data will be fit by an Artificial Neural Network, more specifically a Fully Convolutional Neural Network, which has a relatively low number of parameters and thus (hopefully) generalizes better than a vanilla CNN with a Dense head.\n",
    "\n",
    "The network weights will be optimized by Gradient Descent on the gradients determined by Backpropagation.\n",
    "\n",
    "The task is a 12-clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = LayerStack(stream.input_shape, layers=[\n",
    "    \n",
    "    layers.ConvLayer(nfilters=16, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=32, filterx=5, filtery=5, compiled=True),\n",
    "    layers.PoolLayer(filter_size=2, compiled=True),\n",
    "    layers.Activation(\"relu\"),\n",
    "\n",
    "    layers.ConvLayer(nfilters=stream.NUM_CLASSES, filterx=5, filtery=5, compiled=True),\n",
    "\n",
    "    layers.GlobalAveragePooling(),\n",
    "    layers.Activation(\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training takes about 2-3 hours and the network reaches over 99.9% accuracy on the validation set, which is unnaturally high and is caused probably by validation set which is highly similar to the training set.\n",
    "\n",
    "Below are the parameters for the training. Previous experiments showed that 10 epochs are sufficient to reach convergence on this dataset. The relatively low batch size and high learning rate has ensures the network jumps out of smaller local minima and finds a good optimum with good generalization. Together with the fully convolutional nature of the architecture, this produces sufficient regularization, so no additional regularization was required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Backpropagation(layerstack=stack, cost=\"cxent\", optimizer=optimizers.Adam(LEARNING_RATE))\n",
    "\n",
    "net.fit_generator(stream.iter_subset(\"train\", BATCH_SIZE),\n",
    "                  lessons_per_epoch=stream.steps_per_epoch(\"train\", BATCH_SIZE),\n",
    "                  epochs=10,\n",
    "                  metrics=[\"acc\"],\n",
    "                  validation=stream.iter_subset(\"val\", BATCH_SIZE),\n",
    "                  validation_steps=stream.steps_per_epoch(\"val\", BATCH_SIZE))\n",
    "\n",
    "# Save the weights as NumPy vector.\n",
    "weights = net.get_weights(unfold=True)\n",
    "np.save(\"AIM-SR-weights.npy\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Below we set up some functions to aid testing the network on arbitrary input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_WEIGHTS = \"AIM-SR-weights.npy\"\n",
    "\n",
    "def preprocess_image(image):\n",
    "    x = image / 255.  # Downscale to range 0. - 1.\n",
    "    x = x.transpose((2, 0, 1))  # Convert to channels first\n",
    "    return x[None, ...]  # Add a batch dimension\n",
    "\n",
    "def execute_detection(image):\n",
    "    x = preprocess_image(image)\n",
    "    output = stack.feedforward(x)[0]  # eliminate batch dim\n",
    "    prediction = np.argmax(output, axis=1)\n",
    "    return prediction\n",
    "\n",
    "stack.set_weights(np.load(NETWORK_WEIGHTS), fold=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}